[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/018_blog/tone.html",
    "href": "posts/018_blog/tone.html",
    "title": "Using LLMs to Control Tone in Blog Writing",
    "section": "",
    "text": "Using LLMs to Control Tone in Blog Writing\n\n\nIn this post, I examined how an LLM can be used to rewrite the same content in different tones. Tone control is especially important for blogging, since the same idea can sound academic, casual, or reflective depending on how it is written.\n\n\nRather than asking the model to create new ideas, I used an existing paragraph and tested how different prompts changed the tone while keeping the meaning consistent.\n\n\nThe Graphic\n\n\n\n\n\nOriginal Paragraph\n\n\nLarge language models are becoming increasingly common tools for writing and studying. However, their usefulness depends on how intentionally they are used, especially when clarity and audience matter.\n\n\nPrompt 1: Formal Academic Tone\n\n\n\nRewrite this paragraph in a formal academic tone.\n\n\n\nResponse 1\n\n\n\nLarge language models have become increasingly prevalent tools in academic and professional writing contexts. Their effectiveness, however, is contingent upon intentional and audience-aware use, particularly when clarity is a priority.\n\n\n\nThis version sounded polished and appropriate for a paper, but it felt distant and less personal.\n\n\nPrompt 2: Casual Blog Tone\n\n\n\nRewrite this paragraph in a casual, conversational blog tone.\n\n\n\nResponse 2\n\n\n\nLLMs are becoming a normal part of how people write and study, but how useful they are really depends on how thoughtfully they’re used — especially when you’re trying to communicate clearly with a specific audience in mind.\n\n\n\nThis version felt more approachable and better suited for a blog, even though the core idea stayed the same.\n\n\nReflection\n\n\nThis experiment showed that LLMs are effective tools for adjusting tone without changing meaning. Small prompt changes can dramatically alter how text feels to a reader.\n\n\nFor blogging, this makes LLMs especially useful during revision. Instead of rewriting entire sections manually, I can quickly test different tones and then choose the one that best fits the audience"
  },
  {
    "objectID": "posts/027_blog/Untitled.html",
    "href": "posts/027_blog/Untitled.html",
    "title": "Using LLMs to Summarize Long Blog Drafts",
    "section": "",
    "text": "Using LLMs to Summarize Long Blog Drafts\n\n\nIn this post, I examined how an LLM can be used to summarize a long blog draft into a concise overview. When posts grow longer through revision, it can become difficult to clearly articulate the main takeaway.\n\n\nRather than summarizing manually, I used the model to distill the core argument while preserving the original intent of the post.\n\n\nThe Graphic\n\n\n\n\n\nDraft Excerpt\n\n\nLarge language models can assist writers at multiple stages of the blogging process, from brainstorming ideas to revising drafts. However, their effectiveness depends on how intentionally they are used. Without clear goals, LLM outputs can become unfocused or overly generic, which may weaken the overall message of a post.\n\n\nPrompt\n\n\n\nSummarize this excerpt in one or two sentences while preserving its main point.\n\n\n\nResponse\n\n\n\nLLMs can support blogging at many stages, but their usefulness depends on intentional use. Without clear goals, their outputs may become generic and weaken a post’s overall message.\n\n\n\nThe summary captured the central idea without introducing new claims. It made the main takeaway clearer and more immediately accessible.\n\n\nReflection\n\n\nThis experiment showed how useful LLMs can be for clarifying the core message of a post. Seeing a condensed version helped me evaluate whether the draft was staying focused.\n\n\nFor blogging, this is especially helpful when writing introductions, conclusions, or summaries for readers who may not read the entire post.\n\n\nOverall, this use case reinforced that LLMs are effective tools for synthesis and clarity, not just content generation.\n\ng"
  },
  {
    "objectID": "posts/011_blog/Untitled.html",
    "href": "posts/011_blog/Untitled.html",
    "title": "Battle of the Bots: Putting LLMs to the Test",
    "section": "",
    "text": "When Gemini, GPT, and Perplexity Don’t Agree\n\n\nGraphic\n\n\n\n\nLLM Comparison Graphic\n\n\n\nThe Experiment\n\nWe hear a lot about “AI competition” — but what does it actually look like when three large language models face the same question?\nFor this experiment, I posed identical prompts to ChatGPT, Gemini, and Perplexity.\nEach model had the same goal: explain the ethics of AI creativity in under 100 words.\nHere’s what happened.\n\n\nPrompt 1: “Can machines create ethically?”\n\nGemini wrote like a philosopher — abstract, balanced, and careful not to offend.\n\nChatGPT wrote like a writer — structured, reflective, and humanly cautious.\n\nPerplexity pulled facts — citing researchers and summarizing public debates.\n\nEach was right in its own way. Together, they revealed how “intelligence” reflects perspective.\n\n\n\nPrompt 2: “Describe a future where AIs collaborate with artists.”\nGemini leaned poetic.\nChatGPT built a narrative.\nPerplexity turned into a curator — listing examples and links.\nIt made me realize: the way a model answers tells you what it values.\n\n\n\nTakeaway\nTesting LLMs isn’t just about accuracy — it’s about attitude.\nEach model speaks from a slightly different worldview, like three mirrors reflecting the same question at different angles.\nIf we learn to listen to all three, we don’t just get better answers — we get better questions."
  },
  {
    "objectID": "posts/014_blog/reading.html",
    "href": "posts/014_blog/reading.html",
    "title": "Thinking About Thinking: Early Lessons from A Brief History of Intelligence",
    "section": "",
    "text": "The Evolution of Intelligence\n\n\nThe Image\n\n\n\n\nA Brief History of Intelligence Graphic\n\n\n\nThe Copy Paste\n\nIn A Brief History of Intelligence, the book explains how intelligence didn’t deepen through a single breakthrough, but by adding new mental tools over time — reacting, remembering, planning, imagining. While reading, I started noticing how prompts can nudge AI through similar layers. Some prompts activate basic recall. Others trigger analysis or comparison. Some push toward synthesis or imagination.\nIt made me wonder if prompts serve as small levers that move the AI between these evolutionary “stages” of thinking.\nA simple prompt stays at one level. A more open prompt moves into richer territory.\nTracing the Layers\nSome prompts keep things straightforward:\n“Define this.”\n“List the main points.”\nOthers encourage more complex thinking:\n“Explain how these ideas relate.”\n“What’s the deeper mechanism here?”\nAnd some prompts push into creative or hypothetical reasoning:\n“Imagine how this might have evolved differently…”\n“What would happen if you changed one variable…?”\nThe book shows how intelligence developed by stacking and combining mental abilities. Prompts can guide AI through those same stacked layers.\n\nThe Prompt and Response\n\nPrompt\n\n\nExplain how the evolutionary layers of intelligence described in the book can be mirrored in the way AI responds to different prompts.\n\n\nResponse\n\n\nThe book’s layers — from reaction to learning to imagination — parallel the modes an AI shifts into depending on the prompt. Direct prompts lead to quick, reactive answers. Analytical prompts engage learning-like reasoning. Open-ended or creative prompts activate imaginative patterns. Just as biological intelligence expanded by adding new layers, prompting can guide an AI to use deeper or more complex reasoning.\n\n\n\nReflection\n\nReading the book made me realize prompting isn’t just about extracting information. It’s about choosing which layer of intelligence you want to tap into.\nIf I ask basic questions, I get basic responses. If I ask layered or evolutionary questions, the AI “climbs” to those layers.\nIt’s funny — exploring the evolution of intelligence is helping me evolve how I ask questions too."
  },
  {
    "objectID": "posts/013_blog/Game.html",
    "href": "posts/013_blog/Game.html",
    "title": "AI Showdown: When LLMs Call the Game",
    "section": "",
    "text": "Who Wins the AI World Cup of Predictions?\n\n\nThe Image\n\n\n\n\nLLM Sports Graphic\n\n\n\nThe Setup\n\nWhat happens when you ask three large language models — ChatGPT, Gemini, and Perplexity — to predict the winner of the next World Cup?\nNo training data from the future. No insider stats. Just reasoning, bias, and a little swagger.\n\n\nPrompt 1: “Who’s most likely to win the 2026 World Cup?”\n\nGemini went statistical, breaking down FIFA rankings and team formations.\n\nPerplexity cited sources and quoted betting odds.\n\nChatGPT told a story — complete with drama, rivalries, and heartbreak.\n\nAccuracy? Unknown. Entertainment? 10/10.\n\n\n\nPrompt 2: “Write a game-day speech for the losing team’s coach.”\nHere’s where personality showed up.\n- ChatGPT gave a cinematic locker-room monologue worthy of Netflix.\n- Gemini focused on reflection and leadership growth.\n- Perplexity wrote a two-paragraph press release quoting ESPN.\nIt’s wild how the same models that can code also know the vibe of a halftime speech.\n\n\n\nPrompt 3: “Describe the final goal — if it were scored by an AI.”\nGemini described perfect precision: “An algorithm anticipates the keeper’s hesitation.”\nPerplexity pulled quotes about robotics in sports.\nChatGPT? “A flash of synthetic instinct — data meets destiny.” Goosebumps.\n\n\n\nTakeaway\nLLMs might not know who wins — but they know what it feels like to win.\nSports, it turns out, are the perfect metaphor for testing intelligence: prediction, creativity, and heart.\nMaybe that’s the real contest — not who’s right, but who tells the better story."
  },
  {
    "objectID": "posts/017_blog/Untitled.html",
    "href": "posts/017_blog/Untitled.html",
    "title": "Chaining Prompts for Better Image Generation",
    "section": "",
    "text": "Chaining Prompts for Better Image Generation\n\n\nIn this post, I explored how chaining prompts can improve the quality and specificity of images generated by an LLM. Instead of trying to get the perfect image with a single prompt, I treated image generation as an iterative process.\n\n\nThe goal was to see how providing follow-up instructions based on an initial output could guide the model toward a more precise and intentional result.\n\n\nThe Graphic\n\n\n\n\n\nInitial Prompt\n\n\n\nGenerate an image of a person working on a laptop.\n\n\n\nThe first image produced was technically correct but generic. It showed a person at a desk with a laptop, but there was little context or personality.\n\n\nRefinement Prompt\n\n\n\nRefine the image by placing the person in a college dorm room, working late at night with notes and books visible.\n\n\n\nThis second prompt added more narrative detail. The resulting image included environmental cues like books, papers, and dim lighting, which made the scene feel more realistic and relatable.\n\n\nFinal Prompt\n\n\n\nAdjust the image to feel more informal and lived-in, with a messy desk and casual clothing.\n\n\n\nThe final image felt much closer to the intended idea. Small visual details helped communicate mood and context rather than just subject matter.\n\n\nReflection\n\n\nThis experiment showed that image generation works best when prompts are treated as part of a sequence rather than a one-time command. Each prompt builds on the previous output and helps clarify intent.\n\n\nFor blogging, this approach is especially useful because it allows visuals to align more closely with the narrative of a post. Instead of settling for a generic image, prompt chaining helps create graphics that feel intentional and relevant.\n\n\nOverall, this use case reinforced that iterative prompting is just as important for images as it is for text-based LLM tasks."
  },
  {
    "objectID": "posts/003 _Blog/SportsPrompting.html",
    "href": "posts/003 _Blog/SportsPrompting.html",
    "title": "Sports Prompts and Response Sensitivity",
    "section": "",
    "text": "Prompting\nFor my first test, I wanted to see how small changes in prompts affect how an LLM talks about sports. I used the Philadelphia Eagles as my subject.\n\nPrompt A: “Summarize the Eagles’ 2023 season in one sentence.”\n\nThe one-sentence answer gave a broad evaluation: “The Eagles had a strong season but fell short in the playoffs.”\n\nPrompt B: “Summarize the Eagles’ 2023 season in five bullet points.”\n\nThe five-bullet version highlighted key details:\n\nregular season record\nstandout players\noffensive strengths\ndefensive weaknesses\nplayoff loss\n\n\n\nObservations\nWhat stood out is that the structure of the output wasn’t just shorter or longer, the bullet-point response reorganized the story. This shows that prompt phrasing shapes how an LLM frames sports narratives, which is important if you’re using these models for sports journalism or fan engagement.\n\n\n\nPhiladelphia Eagles"
  },
  {
    "objectID": "posts/002_test_test/test_test.html",
    "href": "posts/002_test_test/test_test.html",
    "title": "Vibe Coding: Finding Flow in Software Development",
    "section": "",
    "text": "When most people think about programming, they picture solving technical problems or shipping features. But there’s another side to coding — one that feels less like work and more like entering a state of flow. This is what I like to call vibe coding.\n\n\n\nVibe coding isn’t about deadlines, bug tickets, or sprint goals. It’s about writing code for the sake of focus, creativity, and momentum.\nIn practice, this might mean: - Putting on music that helps you focus\n- Cleaning up your environment and editor so distractions disappear\n- Building something small, experimental, or even silly — just to see where it goes\nIt’s the programming equivalent of sketching in a notebook: low-pressure, creative, and surprisingly rewarding.\n\n\n\n\nModern development is often fast-paced and outcome-driven. While productivity is important, it can also lead to burnout if every coding session is framed around efficiency. Vibe coding offers a counterbalance — a reminder that programming can be playful and exploratory.\nBenefits include: - Improved creativity: experimenting without constraints often leads to unexpected ideas.\n- Sustained motivation: coding becomes something you look forward to, not just a task list.\n- Skill growth: practice in a low-stakes environment helps sharpen intuition and technique.\n\n\n\n\nIf you want to bring vibe coding into your own workflow, here are some ways to start: 1. Curate a playlist: lo-fi beats, ambient soundtracks, or anything that keeps you engaged without distraction.\n2. Set the stage: dim lights, a clear desk, maybe even a favorite drink nearby.\n3. Pick a fun project: it could be a visualization of your Spotify history, a quirky bot, or a small tool only you will ever use.\n4. Let go of outcomes: the point isn’t to finish — it’s to explore.\n\n\n\n\nCoding doesn’t always have to be serious. Sometimes the best way to unlock creativity, reduce stress, and rediscover joy in programming is to let yourself vibe with the process.\nSo next time you sit down to write code, try setting aside the pressure of productivity. Put on some music, open a blank file, and let the vibes lead you — you might be surprised by what you create."
  },
  {
    "objectID": "posts/002_test_test/test_test.html#what-is-vibe-coding",
    "href": "posts/002_test_test/test_test.html#what-is-vibe-coding",
    "title": "Vibe Coding: Finding Flow in Software Development",
    "section": "",
    "text": "Vibe coding isn’t about deadlines, bug tickets, or sprint goals. It’s about writing code for the sake of focus, creativity, and momentum.\nIn practice, this might mean: - Putting on music that helps you focus\n- Cleaning up your environment and editor so distractions disappear\n- Building something small, experimental, or even silly — just to see where it goes\nIt’s the programming equivalent of sketching in a notebook: low-pressure, creative, and surprisingly rewarding."
  },
  {
    "objectID": "posts/002_test_test/test_test.html#why-vibe-coding-matters",
    "href": "posts/002_test_test/test_test.html#why-vibe-coding-matters",
    "title": "Vibe Coding: Finding Flow in Software Development",
    "section": "",
    "text": "Modern development is often fast-paced and outcome-driven. While productivity is important, it can also lead to burnout if every coding session is framed around efficiency. Vibe coding offers a counterbalance — a reminder that programming can be playful and exploratory.\nBenefits include: - Improved creativity: experimenting without constraints often leads to unexpected ideas.\n- Sustained motivation: coding becomes something you look forward to, not just a task list.\n- Skill growth: practice in a low-stakes environment helps sharpen intuition and technique."
  },
  {
    "objectID": "posts/002_test_test/test_test.html#how-to-try-it-yourself",
    "href": "posts/002_test_test/test_test.html#how-to-try-it-yourself",
    "title": "Vibe Coding: Finding Flow in Software Development",
    "section": "",
    "text": "If you want to bring vibe coding into your own workflow, here are some ways to start: 1. Curate a playlist: lo-fi beats, ambient soundtracks, or anything that keeps you engaged without distraction.\n2. Set the stage: dim lights, a clear desk, maybe even a favorite drink nearby.\n3. Pick a fun project: it could be a visualization of your Spotify history, a quirky bot, or a small tool only you will ever use.\n4. Let go of outcomes: the point isn’t to finish — it’s to explore."
  },
  {
    "objectID": "posts/002_test_test/test_test.html#closing-thoughts",
    "href": "posts/002_test_test/test_test.html#closing-thoughts",
    "title": "Vibe Coding: Finding Flow in Software Development",
    "section": "",
    "text": "Coding doesn’t always have to be serious. Sometimes the best way to unlock creativity, reduce stress, and rediscover joy in programming is to let yourself vibe with the process.\nSo next time you sit down to write code, try setting aside the pressure of productivity. Put on some music, open a blank file, and let the vibes lead you — you might be surprised by what you create."
  },
  {
    "objectID": "posts/006_Blog/LLMdisplays.html",
    "href": "posts/006_Blog/LLMdisplays.html",
    "title": "Cleaning Up Messy Notes with GPT",
    "section": "",
    "text": "Displaying LLM Outputs\n\n\nThe Image\n\n\n\n\nSample output\n\n\n\nThe Copy Paste\n\nHere’s an example of ChatGPT rewriting messy lecture notes\n\n\nThe Organized Notes\nIn class this week my notes came out really messy. Lots of half-sentences and random words. I wanted to see if GPT-5 could make them clearer, so I asked:\n“Rewrite these notes into a clean bullet point list.”\nIt worked well — the output grouped ideas into Definitions, Formulas, and Examples. It was much easier to study from than my original scribbles.\n\n\nCombination of both!\n\nPrompt\n\n&lt;blockquote&gt;\n    Rewrite my messy class notes into a clear bullet point list\n&lt;/blockquote&gt;\n\nResponse\n\n&lt;blockquote&gt;\n    Sure! I’ve cleaned them up and organized them into sections.\n&lt;/blockquote&gt;\n\n\n\nReflection\n\nThis showed me how useful GPT can be for studying. It doesn’t just fix grammar — it makes notes more structured and organized. I’d still double-check details, but as a study helper it really saves time."
  },
  {
    "objectID": "posts/009_blog/booksummary2.html",
    "href": "posts/009_blog/booksummary2.html",
    "title": "Thinking About Thinking: Early Lessons from A Brief History of Intelligence",
    "section": "",
    "text": "From Simple Prompts to Layered Thinking\n\n\nThe Image\n\n\n\n\nThinking about Thinking\n\n\nIn A Brief History of Intelligence, the book talks about how minds evolved through different stages — from reacting, to learning, to eventually imagining. While reading, I started noticing how prompts seem to “nudge” AI into something similar. Some prompts make the model respond quickly and simply. Others make it think more deeply or creatively.\nIt made me wonder if prompts act like small switches that move the AI between these different levels.\nA simple prompt gets a simple answer. A more open prompt gets something more layered.\nLeveling Up the Thinking\nSome prompts stay basic:\n“Define this.”\n“Summarize that.”\nOthers ask for more thought:\n“Explain why this matters.”\n“Compare these ideas.”\nAnd then some prompts push toward imagination:\n“Imagine this…”\n“What if…?”\nThe book describes how humans move between different modes of thinking all the time. Prompts sort of do that for AI — they push it up or down these levels.\n\nThe Prompt and Response\n\nPrompt\n\n\nExplain how different kinds of prompts can activate different levels of reasoning in an AI.\n\n\nResponse\n\n\nDifferent prompts guide the AI into different reasoning patterns. A direct prompt leads to quick recall, while open or imaginative prompts encourage the model to explore broader connections. This mirrors how people shift between recalling, analyzing, or imagining depending on what question they’re asked.\n\n\n\nReflection\n\nThinking about the book made me realize prompting isn’t just about wording things the right way. It’s about choosing what kind of thinking you want to spark.\nIf I ask small questions, I get small answers. If I ask bigger or more flexible questions, the AI tends to follow along.\nIt’s funny — prompting AI is teaching me how I prompt myself too."
  },
  {
    "objectID": "posts/004_Blog/Generatingphotos.html",
    "href": "posts/004_Blog/Generatingphotos.html",
    "title": "Image Generation with ChatGPT",
    "section": "",
    "text": "Generating Images with ChatGPT\nFor this experiment, I tested how an LLM with image-generation can create different visuals from slightly different prompts.\nPrompt A: “Generate an image of a cat sitting on a couch.”\nPrompt B: “Generate an image of a cat lounging on a couch with sunglasses.”\nThe first result was a standard realistic-looking cat image.\nThe second result added humor and personality, showing a playful cat with sunglasses.\nThis shows that small wording changes don’t just shift text output, but also the visual style of generated images. It highlights how careful prompt wording can make the difference between a generic picture and something more creative or entertaining.\n\n\n\nPrompt A: Cat sitting on a couch\n\n\n\n\n\nPrompt B: Cat lounging with sunglasses"
  },
  {
    "objectID": "posts/005_Blog/Writingemails.html",
    "href": "posts/005_Blog/Writingemails.html",
    "title": "Using LLMs to Write Emails",
    "section": "",
    "text": "Displaying LLM Outputs\n\n\nThe Copy Paste\n\n\n\n\nWriting Emails at University\n\n\nHere’s a short example of ChatGPT writing an email\n\n\nThe Email Draft\nThis week I had to email a professor about missing class for a family commitment. Normally, I would just write it myself, but I wanted to see how ChatGPT would handle the situation. I asked:\n“Write me a polite email to my professor explaining I will miss class on Friday due to a family event.”\nThe model produced the following draft:\n\nSubject: Request to Miss Class on Friday\nDear Professor,\nI hope this message finds you well. I am writing to let you know that I will not be able to attend class on Friday due to a family event. I will make sure to catch up on the material covered. Thank you for your understanding.\nSincerely, [Your Name]\n\nI thought this was actually pretty close to what I’d send. The tone was respectful and professional, but it was also a little too generic. I edited it to include the course name and asked if there was anything I should review before the next class.\nThis showed me that while ChatGPT is a good starting point, I still need to personalize the response to sound like me.\n\n\nSome alternatives\n\n\nThe Quote\n\n\nWrite me a polite email to my professor about missing class Of course! Would you like me to keep it formal, or more casual?\n\n\nChanging Background Colors\n\n\nOf course! Would you like me to keep it formal, or more casual?\n\n\nCombination of both!\n\nPrompt\n\n&lt;blockquote&gt;\n    Write me a polite email to my professor about missing class\n&lt;/blockquote&gt;\n\nResponse\n\n&lt;blockquote&gt;\n    Of course! Would you like me to keep it formal, or more casual?\n&lt;/blockquote&gt;\n\n\n\nIntegrating responses\n\nThis week I wanted to explore how GPT-5 handles student communication tasks. We often hear about people using it for essays or coding, but I think everyday uses like emailing professors are just as important.\nWhen I tested it, the response was professional and grammatically correct, but also generic. In practice, I’d use it as a template, then tweak details to make sure it sounds like me.\nThis made me think about how LLMs shape communication: they’re great at giving us a baseline of clarity and politeness, but personalization still matters — especially in academic settings where tone can affect relationships."
  },
  {
    "objectID": "posts/015_blog/ClaudeXChatgpt.html",
    "href": "posts/015_blog/ClaudeXChatgpt.html",
    "title": "Comparing GPT and Claude for Revising Academic Writing",
    "section": "",
    "text": "Comparing GPT and Claude for Revising Academic Writing\n\n\nIn this post, I compared how two different large language models — GPT and Claude — handle the task of revising academic writing. Instead of asking either model to generate new content, I focused on a common student use case: improving clarity and flow in an existing paragraph.\n\n\nThe goal of this experiment was to see how each model balances preserving meaning with improving readability. I used the same input paragraph and the same prompt for both models.\n\n\nThe Graphic\n\n\n\n\n\nThe Original Paragraph\n\n\nThe article argues that media exposure influences attitudes over time, but it does not clearly explain how repeated exposure leads to attitude change. This makes the argument harder to follow because the connection between media use and belief formation is implied rather than explained.\n\n\nThe Prompt\n\n\n\nRevise this paragraph to improve clarity and flow without changing its meaning.\n\n\n\nGPT Response\n\n\n\nThe article argues that media exposure influences attitudes over time, but it does not clearly explain the mechanism through which repeated exposure leads to attitude change. As a result, the argument is harder to follow because the relationship between media use and belief formation is suggested rather than explicitly described.\n\n\n\nGPT’s response stayed very close to the original structure. It focused on tightening phrasing and increasing precision, but it maintained a formal and academic tone throughout.\n\n\nClaude Response\n\n\n\nThe article suggests that media exposure influences attitudes over time, but it does not clearly explain how repeated exposure produces this change. Because this process is left implicit, the connection between media use and belief formation is harder for the reader to follow.\n\n\n\nClaude’s revision was slightly shorter and more conversational. It reorganized ideas more noticeably, which made the paragraph easier to read without oversimplifying the argument.\n\n\nReflection\n\n\nThis comparison showed that both models are useful, but in different ways. GPT was more conservative, making minimal edits that preserved the original structure. Claude was more willing to restructure the paragraph to prioritize readability.\n\n\nFor academic writing, I found Claude more helpful when clarity was the main concern, while GPT felt safer when I wanted minimal changes. This suggests that model choice matters depending on the stage of the writing process.\n\n\nOverall, this experiment reinforced that LLMs work best as revision tools rather than replacements for thinking or argument development."
  },
  {
    "objectID": "posts/016_blog/Prompting.html",
    "href": "posts/016_blog/Prompting.html",
    "title": "Iterating Prompts to Improve Blog Summaries",
    "section": "",
    "text": "Using LLMs to Control Tone in Blog Writing\n\n\nIn this post, I examined how an LLM can be used to rewrite the same content in different tones. Tone control is especially important for blogging, since the same idea can sound academic, casual, or reflective depending on how it is written.\n\n\nRather than asking the model to create new ideas, I used an existing paragraph and tested how different prompts changed the tone while keeping the meaning consistent.\n\n\nThe Graphic\n\n\n\n\n\nOriginal Paragraph\n\n\nLarge language models are becoming increasingly common tools for writing and studying. However, their usefulness depends on how intentionally they are used, especially when clarity and audience matter.\n\n\nPrompt 1: Formal Academic Tone\n\n\n\nRewrite this paragraph in a formal academic tone.\n\n\n\nResponse 1\n\n\n\nLarge language models have become increasingly prevalent tools in academic and professional writing contexts. Their effectiveness, however, is contingent upon intentional and audience-aware use, particularly when clarity is a priority.\n\n\n\nThis version sounded polished and appropriate for a paper, but it felt distant and less personal.\n\n\nPrompt 2: Casual Blog Tone\n\n\n\nRewrite this paragraph in a casual, conversational blog tone.\n\n\n\nResponse 2\n\n\n\nLLMs are becoming a normal part of how people write and study, but how useful they are really depends on how thoughtfully they’re used — especially when you’re trying to communicate clearly with a specific audience in mind.\n\n\n\nThis version felt more approachable and better suited for a blog, even though the core idea stayed the same.\n\n\nReflection\n\n\nThis experiment showed that LLMs are effective tools for adjusting tone without changing meaning. Small prompt changes can dramatically alter how text feels to a reader.\n\n\nFor blogging, this makes LLMs especially useful during revision. Instead of rewriting entire sections manually, I can quickly test different tones and then choose the one that best fits the audience and purpose.\n\n\nOverall, this use case reinforced that LLMs are not just writing tools, but also powerful revision and style-control tools."
  },
  {
    "objectID": "posts/029_blog/multistep.html",
    "href": "posts/029_blog/multistep.html",
    "title": "Using LLMs to Plan Multi-Step Projects",
    "section": "",
    "text": "Using LLMs to Plan Multi-Step Projects\n\n\nIn this post, I examined how an LLM can be used to plan a multi-step project. Complex projects often feel overwhelming because it is difficult to see all the steps at once.\n\n\nRather than asking the model to complete any work directly, I used it as a planning assistant to break a large task into manageable stages.\n\n\nThe Graphic\n\n\n\n\n\nProject Goal\n\n\nCreate a semester-long blog series exploring practical uses of large language models.\n\n\nPrompt\n\n\n\nBreak this project into clear phases with a short description of what happens in each phase.\n\n\n\nResponse\n\n\n\nPhase 1: Define the scope and audience for the blog series.\nPhase 2: Identify and test specific LLM use cases.\nPhase 3: Draft and revise individual blog posts.\nPhase 4: Create visuals and supporting examples.\nPhase 5: Publish posts and reflect on outcomes.\n\n\n\nThe response helped clarify the structure of the project without dictating content. Seeing the process laid out made the task feel more achievable.\n\n\nReflection\n\n\nThis experiment showed that LLMs can be useful tools for organization and planning. Instead of jumping straight into execution, I could step back and think strategically.\n\n\nFor blogging and coursework, this approach helps reduce cognitive overload. The model support"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Rosen’s Explorations with LLMs",
    "section": "",
    "text": "Using LLMs to Plan Multi-Step Projects\n\n\n\nllms\n\nplanning\n\nworkflows\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nThinking About Thinking: Early Lessons from A Brief History of Intelligence\n\n\n\nai\n\nstudying\n\nreading\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nIterating Prompts to Improve Blog Summaries\n\n\n\nllms\n\nprompts\n\nblogging\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Ask Better Follow-Up Questions\n\n\n\nllms\n\nprompts\n\nlearning\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nComparing GPT and Claude for Revising Academic Writing\n\n\n\nllms\n\nwriting\n\ncomparison\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Identify Weak Points in Blog Arguments\n\n\n\nllms\n\nwriting\n\nrevision\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Analyze Sports Game Recaps\n\n\n\nllms\n\nsports\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Test Counterarguments in Blog Writing\n\n\n\nllms\n\nwriting\n\ncritical-thinking\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Turn Lecture Readings into Blog Posts\n\n\n\nllms\n\nstudying\n\nblogging\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Turn Lecture Readings into Blog Posts\n\n\n\nllms\n\nstudying\n\nblogging\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Compare Multiple Drafts of the Same Idea\n\n\n\nllms\n\nwriting\n\nrevision\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nA Brief History of Intelligence: Overview and Key Takeaways\n\n\n\nbooks\n\nintelligence\n\nllms\n\nreading\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nChaining Prompts for Better Image Generation\n\n\n\nllms\n\nimage-generation\n\nprompts\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Turn Bullet Notes into Full Blog Paragraphs\n\n\n\nllms\n\nwriting\n\nblogging\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Simulate Reader Personas\n\n\n\nllms\n\nsimulation\n\nblogging\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Summarize Long Blog Drafts\n\n\n\nllms\n\nwriting\n\nrevision\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Control Tone in Blog Writing\n\n\n\nllms\n\nwriting\n\ntone\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Rewrite Blog Intros for Different Audiences\n\n\n\nllms\n\nwriting\n\nblogging\n\n\n\n\n\n\n\n\n\nDec 18, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nStress Testing the Machines: How LLMs Handle Logic Under Pressure\n\n\n\nai\n\nllm\n\nreasoning\n\n\n\n\n\n\n\n\n\nNov 10, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nBattle of the Bots: Putting LLMs to the Test\n\n\n\nai\n\nllm\n\nevaluation\n\n\n\n\n\n\n\n\n\nNov 9, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nAI Showdown: When LLMs Call the Game\n\n\n\nai\n\nllm\n\nsports\n\n\n\n\n\n\n\n\n\nNov 3, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nThinking About Thinking: The Art of Prompt Framing\n\n\n\nai\n\nprompting\n\ncreativity\n\n\n\n\n\n\n\n\n\nNov 3, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nTesting AI for Business Brainstorms\n\n\n\nllms\n\nbusiness\n\nstudying\n\n\n\n\n\n\n\n\n\nOct 15, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning Up Messy Notes with GPT\n\n\n\nllms\n\nstudying\n\n\n\n\n\n\n\n\n\nSep 19, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to Write Emails\n\n\n\nllms\n\ncommunication\n\nuniversity\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nImage Generation with ChatGPT\n\n\n\nLLMs\n\nImage Generation\n\n\n\n\n\n\n\n\n\nSep 14, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nThinking About Thinking: Early Lessons from A Brief History of Intelligence\n\n\n\nai\n\nstudying\n\nreading\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nSports Prompts and Response Sensitivity\n\n\n\nLLMs\n\nsports\n\nfootball\n\n\n\n\n\n\n\n\n\nSep 7, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nVibe Coding: Finding Flow in Software Development\n\n\n\n\n\n\n\n\nSep 3, 2025\n\n\nAlex Rosen\n\n\n\n\n\n\n\n\n\n\n\n\nThinking About Thinking: Early Lessons from A Brief History of Intelligence\n\n\n\nai\n\nstudying\n\nreading\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nAlex Rosen\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/008_blog/booksummary.html",
    "href": "posts/008_blog/booksummary.html",
    "title": "Thinking About Thinking: Early Lessons from A Brief History of Intelligence",
    "section": "",
    "text": "The Evolution of Intelligence\n\n\nThe Image\n\n\n\n\nA Brief History of Intelligence Graphic\n\n\n\nThe Copy Paste\n\nHere’s what stood out to me from the first few chapters of Max Bennett’s A Brief History of Intelligence — especially his breakdown of how intelligence evolved in stages.\n\n\nThe Early Stages\nBennett starts with a bold idea: that intelligence isn’t just something humans have — it’s something that emerged layer by layer over millions of years.\nHe introduces the five big breakthroughs in cognitive evolution. The first two especially caught my attention:\n\nThe Reflex Agent — basic stimulus–response behavior, like an organism pulling away from danger.\n\nThe Reinforcement Learner — creatures start remembering what worked, adjusting their behavior based on outcomes.\n\nThese sound simple, but Bennett argues that even these small steps set the stage for everything that followed — emotion, planning, and abstract thought.\n\n\nThe Prompt and Response\n\nPrompt\n\n&lt;blockquote&gt;\n    Summarize the first part of *A Brief History of Intelligence* and explain how it connects to AI.\n&lt;/blockquote&gt;\n\nResponse\n\n&lt;blockquote&gt;\n    The early chapters trace how natural intelligence evolved in discrete steps — each solving a problem of survival.  \n    Reflexes led to learning, learning led to modeling the world, and eventually to planning and reasoning.  \n    Bennett’s argument suggests that modern AI mirrors this pattern: large language models are just another “layer” — machines learning by reinforcement and prediction.\n&lt;/blockquote&gt;\n\n\n\nReflection\n\nWhat I like about Bennett’s approach is how biological and computational his story feels at the same time. It makes AI feel less like magic and more like an extension of natural evolution — a continuation of the same logic that built the human brain.\nReading this, I started to think of AI as part of a long timeline, not as something new, but as a new way of organizing old ideas about intelligence.\n\n\n\n\nA Brief History of Intelligence Graphic"
  },
  {
    "objectID": "posts/022_blog/Followups.html",
    "href": "posts/022_blog/Followups.html",
    "title": "Using LLMs to Ask Better Follow-Up Questions",
    "section": "",
    "text": "Using LLMs to Ask Better Follow-Up Questions\n\n\nIn this post, I examined how an LLM can be used not just to answer questions, but to help generate better follow-up questions. Often, the hardest part of learning or writing is knowing what to ask next.\n\n\nInstead of treating the model as a source of final answers, I used it as a tool for identifying gaps, assumptions, and areas that needed clarification.\n\n\nThe Graphic\n\n\n\n\n\nInitial Question\n\n\nHow do large language models generate text that feels coherent?\n\n\nPrompt\n\n\n\nBased on this question, suggest three follow-up questions that would help deepen understanding.\n\n\n\nResponse\n\n\n\nWhat role does training data play in shaping coherence?\nHow do probability and prediction influence sentence structure?\nWhat are the limitations of coherence in LLM-generated text?\n\n\n\nThe suggested questions helped break a broad topic into more focused areas. Each follow-up pointed toward a different dimension of the original question.\n\n\nReflection\n\n\nThis experiment showed that LLMs can be useful thinking partners, not just answer generators. By helping surface better questions, the model supported deeper engagement with the topic.\n\n\nFor blogging and studying, this approach encourages curiosity rather than closure. It keeps the process open-ended and exploratory.\n\n\nOverall, this use case reinforced that good prompting isn’t always about getting better answers — sometimes it’s about learning how to ask better questions."
  },
  {
    "objectID": "posts/023_blog/blogarguments.html",
    "href": "posts/023_blog/blogarguments.html",
    "title": "Using LLMs to Identify Weak Points in Blog Arguments",
    "section": "",
    "text": "Using LLMs to Identify Weak Points in Blog Arguments\n\n\nIn this post, I examined how an LLM can be used to identify weak or unclear parts of a blog argument. When writing, it is often difficult to notice gaps in logic or places where an idea needs more explanation.\n\n\nRather than asking the model to rewrite anything, I used it as a critical reader to point out areas that might confuse or mislead a reader.\n\n\nThe Graphic\n\n\n\n\n\nDraft Paragraph\n\n\nLLMs are useful tools for bloggers because they can help improve writing quality. Many students rely on them, which shows their effectiveness and importance in academic settings.\n\n\nPrompt\n\n\n\nIdentify any weaknesses, vague claims, or areas that need clarification in this paragraph.\n\n\n\nResponse\n\n\n\nThe paragraph makes broad claims about usefulness and effectiveness without providing specific examples or evidence. It would benefit from explaining how writing quality is improved and why student usage demonstrates effectiveness.\n\n\n\nThe response highlighted exactly what was missing: specificity and support. It did not rewrite the paragraph, but it clearly pointed out what needed improvement.\n\n\nReflection\n\n\nThis experiment showed that LLMs can function as helpful critical readers. Instead of accepting text at face value, the model flagged areas where the argument was underdeveloped.\n\n\nFor blogging, this is especially useful during revision. It helps catch weak spots before publishing and encourages stronger, more precise writing.\n\n\nOverall, this use case reinforced that LLMs are valuable not just for generating text, but for strengthening ideas through critique."
  },
  {
    "objectID": "posts/030_blog/sports.html",
    "href": "posts/030_blog/sports.html",
    "title": "Using LLMs to Analyze Sports Game Recaps",
    "section": "",
    "text": "Using LLMs to Analyze Sports Game Recaps\n\n\nIn this post, I examined how an LLM can be used to analyze sports game recaps. Sports articles often focus on scores and highlights, but they can miss broader patterns or strategic takeaways.\n\n\nInstead of reading multiple recaps manually, I used the model to identify recurring themes and key moments from a single game summary.\n\n\nThe Graphic\n\n\n\n\n\nGame Recap Excerpt\n\n\nThe team struggled offensively in the first half but adjusted their strategy after halftime. Strong defensive play and timely scoring allowed them to regain momentum and secure a narrow victory.\n\n\nPrompt\n\n\n\nAnalyze this recap and identify the main turning point and strategic factors that influenced the outcome.\n\n\n\nResponse\n\n\n\nThe main turning point occurred after halftime, when the team adjusted its offensive strategy. Improved defensive performance limited the opponent’s scoring opportunities, while timely offensive plays helped shift momentum and ultimately decide the game.\n\n\n\nThe response clarified the narrative of the game beyond the final score. It highlighted how strategy and momentum played a central role in the outcome.\n\n\nReflection\n\n\nThis experiment showed that LLMs can be useful tools for sports analysis, not just summarization. By focusing on patterns and turning points, the model helped extract insight from a short recap.\n\n\nFor sports blogging or commentary, this approach can support deeper analysis without requiring advanced statistics.\n\n\nOverall, this use case reinforced that LLMs can assist in interpreting sports narratives and identifying meaningful moments within game coverage."
  },
  {
    "objectID": "posts/025_blog/counterarguments.html",
    "href": "posts/025_blog/counterarguments.html",
    "title": "Using LLMs to Test Counterarguments in Blog Writing",
    "section": "",
    "text": "Using LLMs to Test Counterarguments in Blog Writing\n\n\nIn this post, I examined how an LLM can be used to surface potential counterarguments to a blog post. When writing persuasively, it is easy to overlook objections that readers might raise.\n\n\nInstead of using the model to strengthen my own argument directly, I asked it to adopt a skeptical perspective and challenge the main claim.\n\n\nThe Graphic\n\n\n\n\n\nMain Claim\n\n\nLLMs are effective tools for improving blog writing when used intentionally and thoughtfully.\n\n\nPrompt\n\n\n\nList two reasonable counterarguments a skeptical reader might raise against this claim.\n\n\n\nResponse\n\n\n\nOne counterargument is that relying on LLMs may weaken a writer’s original voice and critical thinking skills over time.\nAnother concern is that without careful verification, LLM-generated suggestions could introduce inaccuracies or oversimplifications.\n\n\n\nThe response highlighted concerns I had not fully addressed. It did not dismiss the original claim, but it exposed areas that needed clarification or qualification.\n\n\nReflection\n\n\nThis experiment showed that LLMs can act as productive skeptics. By articulating counterarguments, the model helped stress-test the strength of my reasoning.\n\n\nFor blogging, this is useful before publishing. Anticipating objections makes arguments more balanced and credible.\n\n\nOverall, this use case reinforced that LLMs can support critical thinking by challenging ideas rather than simply reinforcing them."
  },
  {
    "objectID": "posts/024_blog/lectures.html",
    "href": "posts/024_blog/lectures.html",
    "title": "Using LLMs to Turn Lecture Readings into Blog Posts",
    "section": "",
    "text": "Using LLMs to Turn Lecture Readings into Blog Posts\n\n\nIn this post, I examined how an LLM can be used to transform dense lecture readings into accessible blog-style writing. Academic texts often contain valuable ideas, but their structure and language can make them difficult to repurpose.\n\n\nRather than summarizing mechanically, I focused on using the model to help translate academic concepts into clearer, more conversational prose while preserving the original meaning.\n\n\nThe Graphic\n\n\n\n\n\nSource Excerpt\n\n\nThe author argues that media technologies shape not only how information is transmitted, but also how individuals perceive and interpret social reality over time.\n\n\nPrompt\n\n\n\nRewrite this excerpt as a short blog paragraph for a general audience. Keep the core idea the same.\n\n\n\nResponse\n\n\n\nThe author suggests that media doesn’t just deliver information — it actually shapes how people understand the world around them. Over time, the technologies we use influence how we interpret events, ideas, and even social relationships.\n\n\n\nThe response kept the original argument intact while making the language more approachable and easier to follow.\n\n\nReflection\n\n\nThis experiment showed how useful LLMs can be for bridging the gap between academic reading and public-facing writing. Instead of rewriting everything from scratch, I could use the model to help translate ideas into a different register.\n\n\nFor blogging, this makes it easier to connect course material to personal reflection. The model supports clarity without replacing critical thinking.\n\n\nOverall, this use case reinforced that LLMs are effective tools for adapting ideas across contexts while preserving their intellectual core."
  },
  {
    "objectID": "posts/019_blog/tone.html",
    "href": "posts/019_blog/tone.html",
    "title": "Using LLMs to Turn Lecture Readings into Blog Posts",
    "section": "",
    "text": "Using LLMs to Control Tone in Blog Writing\n\n\nIn this post, I examined how an LLM can be used to rewrite the same content in different tones. Tone control is especially important for blogging, since the same idea can sound academic, casual, or reflective depending on how it is written.\n\n\nRather than asking the model to create new ideas, I used an existing paragraph and tested how different prompts changed the tone while keeping the meaning consistent.\n\n\nThe Graphic\n\n\n\n\n\nOriginal Paragraph\n\n\nLarge language models are becoming increasingly common tools for writing and studying. However, their usefulness depends on how intentionally they are used, especially when clarity and audience matter.\n\n\nPrompt 1: Formal Academic Tone\n\n\n\nRewrite this paragraph in a formal academic tone.\n\n\n\nResponse 1\n\n\n\nLarge language models have become increasingly prevalent tools in academic and professional writing contexts. Their effectiveness, however, is contingent upon intentional and audience-aware use, particularly when clarity is a priority.\n\n\n\nThis version sounded polished and appropriate for a paper, but it felt distant and less personal.\n\n\nPrompt 2: Casual Blog Tone\n\n\n\nRewrite this paragraph in a casual, conversational blog tone.\n\n\n\nResponse 2\n\n\n\nLLMs are becoming a normal part of how people write and study, but how useful they are really depends on how thoughtfully they’re used — especially when you’re trying to communicate clearly with a specific audience in mind.\n\n\n\nThis version felt more approachable and better suited for a blog, even though the core idea stayed the same.\n\n\nReflection\n\n\nThis experiment showed that LLMs are effective tools for adjusting tone without changing meaning. Small prompt changes can dramatically alter how text feels to a reader.\n\n\nFor blogging, this makes LLMs especially useful during revision. Instead of rewriting entire sections manually, I can quickly test different tones and then choose the one that best fits the audience and purpose.\n\n\nOverall, this use case reinforced that LLMs are not just writing tools, but also powerful revision and style-control tools."
  },
  {
    "objectID": "posts/020_blog/drafts.html",
    "href": "posts/020_blog/drafts.html",
    "title": "Using LLMs to Compare Multiple Drafts of the Same Idea",
    "section": "",
    "text": "Using LLMs to Compare Multiple Drafts of the Same Idea\n\n\nIn this post, I examined how an LLM can be used to generate and compare multiple drafts of the same idea. This is especially useful during revision, when it can be hard to see alternative ways of expressing the same point.\n\n\nRather than choosing the first output the model produced, I intentionally asked for multiple versions to compare tone, clarity, and emphasis.\n\n\nThe Graphic\n\n\n\n\n\nOriginal Idea\n\n\nLLMs can support writing, but they work best when users remain actively involved in shaping and revising the output.\n\n\nPrompt\n\n\n\nWrite two different versions of this idea, each with a slightly different emphasis.\n\n\n\nResponse 1\n\n\n\nLarge language models can be helpful writing tools, but their value depends on users actively revising and shaping the generated content rather than accepting it as-is.\n\n\n\nThis version emphasized responsibility and caution, framing the LLM as a tool that requires oversight.\n\n\nResponse 2\n\n\n\nWhen used thoughtfully, LLMs can support the writing process by offering drafts that users refine, adapt, and improve through active engagement.\n\n\n\nThis version felt more optimistic and process-oriented, highlighting collaboration between the writer and the model.\n\n\nReflection\n\n\nThis experiment showed that generating multiple drafts can reveal subtle differences in emphasis that are easy to miss when working with a single version.\n\n\nFor blogging, this makes revision feel more exploratory rather than corrective. Comparing drafts helps clarify what I actually want to say, not just how to say it.\n\n\nOverall, this use case reinforced that LLMs are most valuable when they expand the space of possibilities rather than narrowing it too quickly."
  },
  {
    "objectID": "posts/031_blog/book.html",
    "href": "posts/031_blog/book.html",
    "title": "A Brief History of Intelligence: Overview and Key Takeaways",
    "section": "",
    "text": "A Brief History of Intelligence: Overview and Key Takeaways\n\n\nIn this post, I provide an overall overview of A Brief History of Intelligence by Max Bennett and reflect on its central arguments and themes. Rather than focusing on any single chapter, this post synthesizes the book’s broader perspective on how intelligence emerges, evolves, and accumulates over time.\n\n\nBennett’s core contribution is reframing intelligence not as a single human trait, but as a layered process shaped by biology, culture, and technology.\n\n\nThe Graphic\n\n\n\n\n\nCentral Argument\n\n\nIntelligence develops through the accumulation of representational systems, from simple biological signaling to complex symbolic and technological structures.\n\n\nKey Takeaways\n\n\n\nIntelligence is not confined to brains alone, but distributed across systems. Cultural tools like language and writing fundamentally reshape how thinking works. Modern AI represents an extension of this historical process rather than a sudden rupture.\n\n\n\nConnection to LLMs\n\n\n\nLarge language models can be understood as external cognitive systems that store, compress, and reorganize knowledge. Their significance lies less in mimicking human intelligence and more in how they alter the broader ecosystem of thinking.\n\n\n\nThis framing shifts the conversation away from whether AI is “intelligent like humans” and toward how it participates in an ongoing historical pattern of cognitive augmentation.\n\n\nReflection\n\n\nReading A Brief History of Intelligence encouraged me to think of intelligence as something that builds slowly through layers rather than appearing fully formed. This perspective makes current debates about AI feel less unprecedented and more historically grounded.\n\n\nFor understanding LLMs, Bennett’s framework is especially useful. It suggests that the most important questions are not about replacement or competition, but about integration and responsibility.\n\n\nOverall, the book provides a valuable lens for situating modern AI within a much longer story of how humans extend their cognitive capacities through tools, symbols, and shared systems."
  },
  {
    "objectID": "posts/021_blog/notes.html",
    "href": "posts/021_blog/notes.html",
    "title": "Using LLMs to Turn Bullet Notes into Full Blog Paragraphs",
    "section": "",
    "text": "Using LLMs to Turn Bullet Notes into Full Blog Paragraphs\n\n\nIn this post, I examined how an LLM can be used to expand rough bullet-point notes into full blog-style paragraphs. This is a common situation when notes capture ideas but not complete thoughts.\n\n\nRather than asking the model to invent new content, I focused on using it to elaborate on ideas I had already written down.\n\n\nThe Graphic\n\n\n\n\n\nThis image will later show notes transforming into structured prose.\n\n\nOriginal Notes\n\n\nLLMs as writing tools\nHelpful for first drafts\nNeed human revision\nGood for clarity, not ideas\n\n\nPrompt\n\n\n\nTurn these bullet-point notes into a short blog paragraph. Keep the meaning the same.\n\n\n\nResponse\n\n\n\nLLMs can be useful writing tools, especially when creating first drafts. They work well for turning rough ideas into clearer sentences, but they still require human revision to ensure accuracy and voice. Rather than generating ideas on their own, these models are most effective when used to clarify and organize thoughts the writer already has.\n\n\n\nThe response expanded the notes into a coherent paragraph without introducing new ideas. It preserved the original intent while improving flow and readability.\n\n\nReflection\n\n\nThis experiment showed how effective LLMs can be at moving from notes to prose. Instead of staring at a blank page, I could use my existing notes as input and quickly generate a usable draft.\n\n\nFor blogging, this makes the transition from thinking to writing much smoother. The model helps fill in connective language while leaving the core ideas intact.\n\n\nOverall, this use case reinforced that LLMs are strongest when they help expand and refine ideas rather than replace them."
  },
  {
    "objectID": "posts/028_blog/personas.html",
    "href": "posts/028_blog/personas.html",
    "title": "Using LLMs to Simulate Reader Personas",
    "section": "",
    "text": "Using LLMs to Simulate Reader Personas\n\n\nIn this post, I examined how an LLM can be used to simulate different reader personas. Instead of guessing how a post might be received, I used the model to respond as if it were different types of readers.\n\n\nThis approach treats the LLM as a stand-in for audience perspectives rather than a writing tool.\n\n\nThe Graphic\n\n\n\n\n\nBlog Excerpt\n\n\nThis blog argues that large language models are most effective when used as collaborative tools rather than replacements for human thinking.\n\n\nPrompt\n\n\n\nRespond to this paragraph as three different readers: a skeptical student, an enthusiastic technologist, and a cautious educator.\n\n\n\nResponse\n\n\n\nSkeptical student: This sounds reasonable, but I’m not convinced that collaboration actually prevents over-reliance on AI.\nEnthusiastic technologist: This aligns perfectly with how I see AI fitting into creative workflows — as a partner, not a replacement.\nCautious educator: The emphasis on collaboration is important, but I would want clearer boundaries around appropriate use.\n\n\n\nThe responses highlighted how the same idea could be interpreted differently depending on background, expectations, and concerns.\n\n\nReflection\n\n\nThis experiment showed that LLMs can be useful tools for anticipating audience reactions. Instead of imagining responses abstractly, I could quickly explore multiple perspectives.\n\n\nFor blogging, this helps refine arguments before publishing. It reveals potential misunderstandings or objections that might not be obvious otherwise.\n\n\nOverall, this use case reinforced that LLMs can support audience analysis and empathy, not just content creation."
  },
  {
    "objectID": "posts/010_blog/Thinkingaboutthinking.html",
    "href": "posts/010_blog/Thinkingaboutthinking.html",
    "title": "Thinking About Thinking: The Art of Prompt Framing",
    "section": "",
    "text": "How the Shape of a Question Shapes the Mind\n\n\nThe Image\n\n\n\n\nPrompt Engineering Graphic\n\n\n\nThe Copy Paste\n\nWhen we talk about AI “prompting,” we usually mean typing an instruction and waiting for a response.\nBut if we zoom out, prompting is something much older — it’s the art of asking the right kind of question to elicit intelligence, whether from humans or machines.\nThe way we frame a prompt determines not only the answer we get, but the kind of thinking that happens inside the system.\nA good prompt doesn’t just extract information — it creates a cognitive context.\n\n\nFraming vs. Instructing\nWhen we instruct, we tell an AI (or a person) what to do.\nWhen we frame, we shape the space in which ideas emerge.\nFor example: - “Write a poem about the ocean.” → A task.\n- “If the ocean could speak, what would it tell the moon?” → A frame.\nThe second prompt engages metaphor, perspective, and imagination.\nIt’s not longer a command — it’s a mental environment.\nAI systems, especially large language models, respond dramatically to framing.\nThey adapt tone, scope, and depth based on subtle contextual cues — mirroring how human creativity responds to open-ended or constraining situations.\n\n\nThe Prompt and Response\n\nPrompt\n\n\nExplain how “prompt framing” can lead to more original or creative outputs from an AI system.\n\n\nResponse\n\n\nPrompt framing encourages AI to explore conceptual associations rather than literal completions.\nBy shifting tone or perspective, we engage different latent regions in the model’s memory — the linguistic equivalent of creative mood lighting.\nThis mirrors how human brainstorming works: the broader the mental space, the more novel the idea.\n\n\n\n\nReflection\n\nReading about prompt engineering often feels like reading a user manual.\nBut prompt framing feels more like reading philosophy — it’s about how a mind is guided by questions.\nI’ve noticed that when I write prompts like “explain,” I get information.\nWhen I write prompts like “imagine,” I get insight.\nIt makes me think that prompting is really a mirror — showing us how we guide our own thinking.\nIn a sense, good prompting isn’t about controlling AI.\nIt’s about collaborating with it — setting up conditions where both the human and the model can think more creatively.\n\n\n\n\nPrompt Engineering Graphic"
  },
  {
    "objectID": "posts/012_blog/Stresstesting.html",
    "href": "posts/012_blog/Stresstesting.html",
    "title": "Stress Testing the Machines: How LLMs Handle Logic Under Pressure",
    "section": "",
    "text": "When Logic Gets Messy: Who Thinks Best Under Pressure?\n\n\nPressure\n\n\n\n\nLLM Reasoning Graphic\n\n\n\nThe Challenge\n\nNot all prompts are polite.\nSome are paradoxes, traps, or puzzles that force models to think rather than recite.\nFor this test, I gave Gemini, Perplexity, and ChatGPT a series of reasoning puzzles and time-sensitive logic questions — the kind that break even strong systems.\n\n\nPrompt 1: “A train leaves Chicago traveling east at 60 mph…”\nClassic word problem.\n- Gemini calculated step-by-step but sometimes over-explained.\n- ChatGPT solved it correctly, but simplified too fast.\n- Perplexity looked up real-world examples of train schedules (!).\nIt turns out: accuracy wasn’t the point — composure was.\n\n\n\nPrompt 2: “Explain this paradox in one sentence: The more you know, the less you understand.”\n\nGemini turned poetic again, writing about humility.\n\nChatGPT defined the epistemological paradox directly.\n\nPerplexity summarized an academic article by a philosopher named Laozi (yes, really).\n\nEach model exposed a different form of reasoning: metaphorical, logical, and empirical.\n\n\n\nTakeaway\nTesting reasoning in LLMs isn’t about who’s right — it’s about how they break.\nWhen the logic gets messy, style becomes signal:\nthe poetic thinker, the rationalizer, and the fact-checker all reveal what “intelligence” really means."
  },
  {
    "objectID": "posts/007_blog/Businessbrainstorm.html",
    "href": "posts/007_blog/Businessbrainstorm.html",
    "title": "Testing AI for Business Brainstorms",
    "section": "",
    "text": "AI as a Business Partner\n\n\nThe Image\n\n\n\n\nTesting AI for Business Brainstorms\n\n\n\nThe Copy Paste\n\nHere’s an example of GPT-5 helping me generate and refine startup ideas during a study session.\n\n\nThe Experiment\nIn a group project for my business class, we had to come up with a new product idea and outline a marketing plan.\nI wondered: could GPT help us think like entrepreneurs, not just write like one?\nSo I asked:\n“Generate 5 startup ideas for college students that solve everyday problems, then outline target customers and pricing strategies.”\nThe result wasn’t perfect — some ideas were unrealistic — but others felt shockingly plausible. It even categorized them by business model and market segment.\n\n\nThe Prompt and Response\n\nPrompt\n\n    Generate 5 startup ideas for college students that solve everyday problems. For each, describe the target customer, value proposition, and basic pricing model.\n\nResponse\n\n    1. DormChef – A meal-prep subscription for students with limited kitchen space. &lt;br&gt;\n    2. TextSwap – Peer-to-peer textbook rental app. &lt;br&gt;\n    3. CampusConnect – Local networking platform for side hustles and projects. &lt;br&gt;\n    4. StudyBuddy AI – Personalized study partner using adaptive learning. &lt;br&gt;\n    5. GreenMove – Shared transport service for moving between dorms sustainably.\n\n\n\nReflection\n\nThis test showed me how AI can support business ideation — not by replacing creativity, but by accelerating it.\nGPT acts like a brainstorming partner that throws ideas at the wall fast, letting you decide which ones to keep.\nIt’s not about trusting the output blindly, it’s about using it to think faster and broader.\n\n\n\n\nTesting AI for Business Brainstorms"
  },
  {
    "objectID": "posts/026_blog/Audiences.html",
    "href": "posts/026_blog/Audiences.html",
    "title": "Using LLMs to Rewrite Blog Intros for Different Audiences",
    "section": "",
    "text": "Using LLMs to Rewrite Blog Intros for Different Audiences\n\n\nIn this post, I examined how an LLM can be used to rewrite the introduction of a blog post for different audiences. The opening of a blog is especially important, since it sets expectations and determines whether a reader continues.\n\n\nRather than changing the main idea of the post, I focused on adjusting how the introduction was framed depending on who the audience was.\n\n\nThe Graphic\n\n\n\n\n\nOriginal Introduction\n\n\nLarge language models are becoming popular tools for writing, but many bloggers are unsure how to use them effectively without losing their own voice.\n\n\nPrompt\n\n\n\nRewrite this introduction for three audiences: students, professional writers, and casual readers.\n\n\n\nResponse\n\n\n\nFor students: Many students are starting to use LLMs for writing, but it can be hard to know how to use them responsibly while still learning how to write on your own.\nFor professional writers: As LLMs enter professional writing workflows, writers face new questions about how to integrate these tools without compromising voice or quality.\nFor casual readers: AI tools are showing up everywhere in writing, but a lot of people aren’t sure how to use them without sounding less like themselves.\n\n\n\nThe responses showed how the same core idea could be framed differently depending on audience expectations, tone, and familiarity with the topic.\n\n\nReflection\n\n\nThis experiment showed how useful LLMs can be for adapting content without changing meaning. Instead of rewriting introductions from scratch, I could quickly explore multiple audience-focused versions.\n\n\nFor blogging, th"
  }
]